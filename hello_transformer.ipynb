{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 이해하기\n",
    "\n",
    "Query, Key, Value 직관적으로 이해하기\n",
    "\n",
    "📖 1-1. 현실에서의 예시\n",
    "\n",
    "\t📌 비유: 도서관에서 책을 찾는 과정\n",
    "\t\t•\tQuery (질의) = 사용자가 찾고 싶은 키워드 (“딥러닝 책”)\n",
    "\t•\tKey (키) = 도서관의 책 제목들\n",
    "\t•\tValue (값) = 해당하는 책의 실제 내용\n",
    "\n",
    "과정:\n",
    "\t1.\t사용자가 “딥러닝 책”을 찾고 싶음 (Query)\n",
    "\t2.\t도서관 시스템이 책 제목(Key)과 Query를 비교하여 가장 관련 있는 책을 찾음\n",
    "\t3.\t해당하는 책의 내용(Value)을 제공\n",
    "\n",
    "💡 결론: Query와 Key를 비교하여 가장 관련 있는 Value를 가져오는 것이 어텐션 메커니즘이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Query, Key, Value를 정의\n",
    "Q = torch.tensor([[1.0, 0.0, 1.0]])  # (1, 3) 크기의 행렬\n",
    "K = torch.tensor([[1.0, 0.0, 1.0], \n",
    "                  [0.0, 1.0, 1.0], \n",
    "                  [1.0, 1.0, 0.0]])  # (3, 3)\n",
    "V = torch.tensor([[10.0, 0.0], \n",
    "                  [0.0, 10.0], \n",
    "                  [5.0, 5.0]])  # (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "scores = torch.matmul(Q, K.T)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5761, 0.2119, 0.2119]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "attention_weights = F.softmax(scores, dim=-1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.8209, 3.1791]])\n"
     ]
    }
   ],
   "source": [
    "output = torch.matmul(attention_weights, V)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0776, -0.3397,  0.3057, -0.3662],\n",
      "        [-0.3474,  0.4001, -0.3288,  0.0862],\n",
      "        [ 0.3752,  0.4758, -0.3547, -0.0924],\n",
      "        [ 0.2171, -0.1210, -0.2775, -0.2962]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "\n",
    "# 예제: 각 단어가 4차원 벡터로 표현됨 (d_model=4)\n",
    "x1 = torch.tensor([1.0, 0.0, 1.0, 0.5])  # \"나는\"\n",
    "x2 = torch.tensor([0.5, 1.0, 0.0, 1.0])  # \"사과\"\n",
    "x3 = torch.tensor([1.0, 1.0, 0.5, 0.0])  # \"를\"\n",
    "x4 = torch.tensor([0.0, 1.0, 1.0, 0.5])  # \"먹었다\"\n",
    "\n",
    "X = torch.stack([x1, x2, x3, x4])  # (4, 4)\n",
    "# print(\"입력 임베딩 (X):\\n\", X)\n",
    "\n",
    "# 가중치 행렬 정의 (4x4 크기, 학습 가능)\n",
    "W_q = nn.Linear(4, 4)  # W_query\n",
    "W_k = nn.Linear(4, 4)  # W_key\n",
    "W_v = nn.Linear(4, 4)  # W_value\n",
    "\n",
    "# Query, Key, Value 생성\n",
    "Q = W_q(X)\n",
    "K = W_k(X)\n",
    "V = W_v(X)\n",
    "\n",
    "# print(\"Query (Q):\\n\", Q)\n",
    "# print(\"Key (K):\\n\", K)\n",
    "# print(\"Value (V):\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 일상 비유 시각화: 도서관에서 책 찾기\n",
    "print(\"===== 도서관에서 책 찾기 비유 =====\")\n",
    "print(\"Query (질문): '인공지능에 관한 책 있나요?'\")\n",
    "print(\"Keys (책 제목들):\")\n",
    "books = [\"파이썬 기초\", \"인공지능 입문\", \"웹 개발 기초\", \"인공지능과 딥러닝\", \"자료구조\"]\n",
    "for i, book in enumerate(books):\n",
    "    print(f\"  Key {i+1}: '{book}'\")\n",
    "\n",
    "# 유사도 계산 (간단히 단어 매칭으로)\n",
    "similarity = []\n",
    "query = \"인공지능\"\n",
    "for book in books:\n",
    "    # 간단한 유사도: 제목에 '인공지능'이 포함되면 높은 점수\n",
    "    score = 1.0 if query in book else 0.1\n",
    "    similarity.append(score)\n",
    "\n",
    "print(\"\\n유사도 점수:\")\n",
    "for i, (book, score) in enumerate(zip(books, similarity)):\n",
    "    print(f\"  '{book}': {score:.1f}\")\n",
    "\n",
    "# 소프트맥스로 가중치 변환\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / exp_x.sum()\n",
    "\n",
    "weights = softmax(similarity)\n",
    "print(\"\\n가중치 (소프트맥스 적용):\")\n",
    "for i, (book, weight) in enumerate(zip(books, weights)):\n",
    "    print(f\"  '{book}': {weight:.2f}\")\n",
    "\n",
    "# Values (책 내용 - 간략히)\n",
    "values = [\n",
    "    \"파이썬 문법과 기초 개념\",\n",
    "    \"인공지능의 개념과 역사\",\n",
    "    \"HTML, CSS, JavaScript 기초\",\n",
    "    \"딥러닝 알고리즘과 신경망 구조\",\n",
    "    \"알고리즘과 데이터 구조 개념\"\n",
    "]\n",
    "\n",
    "# 가중 평균으로 최종 응답 계산\n",
    "response = \"\"\n",
    "for weight, value in zip(weights, values):\n",
    "    response += f\"{weight:.2f} × '{value}'\\n\"\n",
    "\n",
    "print(\"\\n가중 평균된 응답 (어텐션의 결과):\")\n",
    "print(response)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(books, weights, color='skyblue')\n",
    "plt.title('Query \"인공지능\"에 대한 각 Key의 가중치')\n",
    "plt.ylabel('가중치')\n",
    "plt.ylim(0, 0.7)\n",
    "for i, w in enumerate(weights):\n",
    "    plt.text(i, w + 0.02, f'{w:.2f}', ha='center')\n",
    "plt.tight_layout()\n",
    "\n",
    "# 2. 실제 벡터로 구현하는 간단한 셀프 어텐션\n",
    "print(\"\\n\\n===== 벡터로 구현하는 간단한 셀프 어텐션 =====\")\n",
    "\n",
    "# 간단한 문장의 단어 벡터 (가상의 2차원 임베딩)\n",
    "words = [\"나는\", \"인공지능을\", \"공부한다\"]\n",
    "word_vectors = np.array([\n",
    "    [1, 1],    # \"나는\"\n",
    "    [2, 5],    # \"인공지능을\" \n",
    "    [1, 3]     # \"공부한다\"\n",
    "])\n",
    "\n",
    "print(\"단어:\", words)\n",
    "print(\"단어 벡터:\\n\", word_vectors)\n",
    "\n",
    "# 간단한 가중치 행렬 (실제로는 학습됨)\n",
    "np.random.seed(42)  # 재현성을 위한 시드 설정\n",
    "W_query = np.array([[1, 0], [0, 1]])   # 간단히 항등 행렬로 시작\n",
    "W_key = np.array([[1.2, 0.1], [0.1, 0.9]])\n",
    "W_value = np.array([[0.8, 0.3], [0.2, 1.1]])\n",
    "\n",
    "# Q, K, V 계산\n",
    "Q = np.dot(word_vectors, W_query)\n",
    "K = np.dot(word_vectors, W_key)\n",
    "V = np.dot(word_vectors, W_value)\n",
    "\n",
    "print(\"\\nQuery 벡터:\")\n",
    "for word, q in zip(words, Q):\n",
    "    print(f\"  {word}: {q}\")\n",
    "\n",
    "print(\"\\nKey 벡터:\")\n",
    "for word, k in zip(words, K):\n",
    "    print(f\"  {word}: {k}\")\n",
    "\n",
    "print(\"\\nValue 벡터:\")\n",
    "for word, v in zip(words, V):\n",
    "    print(f\"  {word}: {v}\")\n",
    "\n",
    "# 어텐션 스코어 계산 (Q·K^T)\n",
    "scores = np.dot(Q, K.T)\n",
    "print(\"\\n어텐션 스코어:\")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"  {word}:\")\n",
    "    for j, other_word in enumerate(words):\n",
    "        print(f\"    -> {other_word}: {scores[i, j]:.2f}\")\n",
    "\n",
    "# 스케일링\n",
    "d_k = K.shape[1]  # key의 차원\n",
    "scaled_scores = scores / np.sqrt(d_k)\n",
    "print(\"\\n스케일링된 어텐션 스코어 (√d_k={:.2f}로 나눔):\".format(np.sqrt(d_k)))\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"  {word}:\")\n",
    "    for j, other_word in enumerate(words):\n",
    "        print(f\"    -> {other_word}: {scaled_scores[i, j]:.2f}\")\n",
    "\n",
    "# 소프트맥스 적용\n",
    "attention_weights = softmax(scaled_scores)\n",
    "print(\"\\n어텐션 가중치 (소프트맥스 적용):\")\n",
    "for i, word in enumerate(words):\n",
    "    print(f\"  {word}:\")\n",
    "    for j, other_word in enumerate(words):\n",
    "        print(f\"    -> {other_word}: {attention_weights[i, j]:.2f}\")\n",
    "\n",
    "# 가중치와 Value 행렬 곱\n",
    "output = np.dot(attention_weights, V)\n",
    "print(\"\\n최종 출력:\")\n",
    "for word, o in zip(words, output):\n",
    "    print(f\"  {word}: {o}\")\n",
    "\n",
    "# 시각화 - 어텐션 맵\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.imshow(attention_weights, cmap='Blues')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(words)), words)\n",
    "plt.yticks(np.arange(len(words)), words)\n",
    "plt.title('셀프 어텐션 가중치')\n",
    "\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        plt.text(j, i, f'{attention_weights[i, j]:.2f}', \n",
    "                 ha='center', va='center', \n",
    "                 color='white' if attention_weights[i, j] > 0.5 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# 3. 실제 트랜스포머에서의 멀티헤드 어텐션 개념 설명\n",
    "print(\"\\n\\n===== 멀티헤드 어텐션의 의미 =====\")\n",
    "print(\"멀티헤드는 여러 관점으로 데이터를 보는 것과 같습니다:\")\n",
    "print(\"- 헤드 1: 문법적 관계에 집중\")\n",
    "print(\"- 헤드 2: 의미적 관계에 집중\")\n",
    "print(\"- 헤드 3: 주제별 연관성에 집중\")\n",
    "print(\"- 등등...\")\n",
    "print(\"\\n서로 다른 헤드의 결과를 합쳐서 더 풍부한 표현을 얻을 수 있습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
